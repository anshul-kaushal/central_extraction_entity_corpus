{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69a3123-6003-4f68-909f-138f55122a6c",
   "metadata": {},
   "source": [
    "# Building an annotated corpus for main-entity-extraction systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34673350-853f-42e7-b9c1-84ef19d11165",
   "metadata": {},
   "source": [
    "## Source of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26892a7b-885b-441d-9431-a92d612e6039",
   "metadata": {},
   "source": [
    "The source of the data that we will be using is the [**The Guardian Open Platform**](https://open-platform.theguardian.com/) website. The Open Platform is a public web service that provides access to all the content generated by The Guardian. The Guardian, originally founded as The Manchester Guardian, back in 1821, is a British Daily Newspaper. As a news platform, it contains humongous amounts of text data that can be used to facilitate advanced research in the field of computational linguistics. To get access to its data, the Open Platform caters an application programming interface that contains articles, videos, images dating back to 1999. Easy access to a large amount of data (1.9 million content pieces) coupled with an easy to work-with API makes it a good option to get started on building this corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb328f-1a0f-49d1-9315-92017142fd53",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f516211-284a-4486-91b8-40d49271f9af",
   "metadata": {},
   "source": [
    "- The API provides access to all kinds of data that can be found in a news website: ranging from images, articles to videos, crossword puzzles etc. However, as stated earlier, we would be working with news articles in particular. \n",
    "- We'll be working with news articles specifically written in English. Primarily, The Guardian writes in English but more importantly, English is easier to work with, comprehend and annotate in. \n",
    "- The news articles that we're using to build this corpus contains articles written by a diverse range of journalists and not a single person in particular. This allows us to work with different writing styles and explore a diverse range of semantic and syntactic information as well. \n",
    "- The documents are extremely long and varying in length with articles (just the content excluding all the metadata) ranging anywhere from 400-4000 words. Once we kick-off the project workflow, we'll also try to identify if we might benefit more from working with documents that are less diverse in length and more uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a9971-2df4-471f-8553-b883cdfe2c2d",
   "metadata": {},
   "source": [
    "## News of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c7d53-278a-4b95-ad9e-d8c78deef80c",
   "metadata": {},
   "source": [
    "The data, once fetched, can be retrieved in Python in a JSON format as demonstrated in the POC document. One of the keys in the JSON document is 'topic', which categorizes news articles into politics, sports, opinions etc. We decided to work with categories that are mostly entity oriented and not abstract in structure. Thus we filtered in categories like: Politics, Science, Business, Film and Technology and left the rest out. Further discussions might lead to inclusion/exclusion of different categories as well.\n",
    "\n",
    "As demonstrated in the POC document, by utilizing the well structured JSON formatted documents, we've easily identified and extracted our news categories of interest. Given how vast the news catalogue is in general, as well as for a newspaper this popular, even after filtering out  most of the categories, we've been able to fetch more than 1500 articles just for a period of 45 days (ranging from early-January 2022 to mid-February 2022). This indicates that our data source has enough textual information required to build a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2c6fe-1dcb-4de1-93cf-e050111406d0",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5fb86-4f28-4d45-8422-49889a48f3cd",
   "metadata": {},
   "source": [
    "### Structure\n",
    "\n",
    "Tentatively, we have decided to go with a tabular structure for the corpus. Each news article would be a single row or an example. Along with the text as a label, we've decided to include 'topics' metadata to analyze how different categories of news might affect the type of entity recognized as the central piece of information in the news text. We've decided to eliminate headlines or title of the news article, to annotate the main entity of the text based solely on the text itself. This will help us to create a corpus that can be further used to extract useful information from online comments, tweets, facebook posts etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80b3c1-a1db-4e9c-ae96-89e029487494",
   "metadata": {},
   "source": [
    "### Annotation\n",
    "\n",
    "As has been made abundantly clear by now, we're trying to extract the main entity of the piece of information available to be analyzed. For example, a news article written about `Microsoft`, might extract `Microsoft` as the central entity of the piece of text, depending on the context. This is where having different categories of news comes into play. This can help us analyze how different types of entities (person, organization, location) are distributed with respect to the different categories of the news. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63754e7-680c-4bf5-8d5d-56d48911c423",
   "metadata": {},
   "source": [
    "### Format\n",
    "\n",
    "The format of the corpus, as demonstrated by the POC is a tab-seperated value file with labels 'Topic' and 'Text' corresponding to the topic of the news article and the entire text of the news article (excluding author, title etc.) respectively. This can be demonstrated as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9469faa-d13e-4070-9673-934451424944",
   "metadata": {},
   "source": [
    "Topic\tText\n",
    "Business\tTreasury officials have quietly introduced a new “super tax” to deter energy company owners from cashing out lucrative contracts for gas bought in advance before leaving their supply business to go under. The government quickly pushed through the new laws late last week to counter industry concerns that Stephen Fitzpatrick, the founder of Ovo Energy, could use his almost two-thirds stake in the company to liquidate its long-term gas contracts and exit the supply market with a hefty profit. Energy industry sources believe this loophole may have already been used by BP, which owned almost a quarter of Pure Planet before pulling the plug on the challenger brand last October. It is understood to have sold the energy it bought in advance, using the profit to pay back loans to BP. Under the government’s new super-levy, called the public interest business protection tax, a 75% tax will be imposed on any future windfall that an energy company shareholder could hope to make by cashing out gas contracts while leaving millions without a supplier. Many of the suppliers that have gone bust did not have long-term energy contracts, known as hedges, in place. However, those that did buy gas in advance, before markets reached record highs last year, have found the value of the contracts has rocketed. Ovo Energy said the new levy had its support, and that it had “been clear for some time that action was needed” to help protect households and taxpayers. The Guardian understands that the tax was ushered in to block any financial incentive for energy company owners to make a fortune off the loophole, and to provide a “safety net” for household bill payers and the public purse that would otherwise have to bail out customers left behind. Consumers in England, Scotland and Wales could already be on the hook for a total of £3.2bn to cover the cost of finding a new supplier for the 2 million households affected by the collapse of energy suppliers since the start of September, and for the further 1.7 million customers of Bulb Energy, who have been handed to a special administrator appointed to handle the large-scale collapse. Treasury documents said there was “a potential risk” that some energy suppliers and their shareholders might monetise large profits on assets such as hedges “for their own benefit, leaving the government and ultimately the wider public to be landed with the costs of ensuring the continuity of supplies of energy to customers”. “The government considers it unacceptable that, whilst consumers across the UK face rising energy bills, it would be possible for some companies and their shareholders to exploit this situation to the detriment of taxpayers and end consumers,” the documents added. A spokesperson for Ovo Energy said: “We support this, and other recent announcements intended to strengthen the resilience of the UK energy retail market.” However, the company added that it was “still not clear on what – if anything – the government will do to help support vulnerable customers” ahead of the looming hike to the UK’s energy cap which is expected next week. Energy bills are expected to soar to almost £2,000 a year from April under the regulator’s energy price cap – up from an average of £1,277 this winter – in the steepest bill increase since the cap was introduced in 2019. The government has been in talks to soften the blow to households but no measures have been agreed. “We hope this announcement is a sign of further initiatives to come to resolve the current energy crisis,” the Ovo spokesperson said."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374cb50f-66c4-4a34-93ea-740d06a2315e",
   "metadata": {},
   "source": [
    "### Our Vision\n",
    "\n",
    "I was browsing Twitter the other day and was impressed by the feature where it let's us browse the trending tweets corresponding to our favorite celebrities, key politicians and iconic sportsmen. Our vision for the corpus is consolidated around the similar grounds. This corpus can be used to train machine learning models to identify central entities in a given piece of information across various online platforms. This can help us not only to categorize information better, but also make information retrieval more efficient. Google clearly uses this kind of technology to recommend articles to its users based on their search preferences. Moreover, incorporating certain categories of news also helps us to inspect how different categories of news correspond to different categories of entities. Since, it is an abstract idea thus far, as we go further into developing our corpus, we can incorporate other elements to our corpus with the intention of driving further research into the field of computational linguistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
