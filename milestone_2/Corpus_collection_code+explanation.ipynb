{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de72181-fa30-4016-9527-df134e06d9c7",
   "metadata": {},
   "source": [
    "# Corpus collection code & Corpus + explanation\n",
    "\n",
    "We have extracted the Film articles that are published between 2021-01-01 and 2021-12-31 from [The Guardian OpenPlatform](https://open-platform.theguardian.com).\n",
    "\n",
    "## Instruction\n",
    "- Run the code chunks below to obtain the corpus.tsv file that contains 500 Film articles which are less than 1000 words from the Guardian API.\n",
    "- If no file is created after running the code, it is very likely that the key has reached its limits, please register for a developer key [here](https://open-platform.theguardian.com/access/), and replace the **MY_API_KEY** variable with your key.\n",
    "- Stop and restart: The current request progress and data is saved, therefore it can be interrupted and restart with out losing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3e8c30-e02e-4e43-8f19-0f4484cc6677",
   "metadata": {},
   "source": [
    "## 1. Corpus collection code\n",
    "This code chunk shows the main process of retrieving Film articles using Python Requests is to make HTTP requests to the Guardian API for every date between the start and end date (2021-01-01 to 2021-12-31).    \n",
    "It can be interrupted and restart without losing the current request progress and the data. The current progress is save in the *progress_tracker.txt*, and the raw fetched data is saved in *progress_file.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4c2a24-5cd4-41e6-9c0b-d2725d65c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01\n",
      "2021-01-02\n",
      "2021-01-03\n",
      "2021-01-04\n",
      "2021-01-05\n",
      "2021-01-06\n",
      "2021-01-07\n",
      "2021-01-08\n",
      "2021-01-09\n",
      "2021-01-10\n",
      "2021-01-11\n",
      "2021-01-12\n",
      "2021-01-13\n",
      "2021-01-14\n",
      "2021-01-15\n",
      "2021-01-16\n",
      "2021-01-17\n",
      "2021-01-18\n",
      "2021-01-19\n",
      "2021-01-20\n",
      "2021-01-21\n",
      "2021-01-22\n",
      "2021-01-23\n",
      "2021-01-24\n",
      "2021-01-25\n",
      "2021-01-26\n",
      "2021-01-27\n",
      "2021-01-28\n",
      "2021-01-29\n",
      "2021-01-30\n",
      "2021-01-31\n",
      "2021-02-01\n",
      "2021-02-02\n",
      "2021-02-03\n",
      "2021-02-04\n",
      "2021-02-05\n",
      "2021-02-06\n",
      "2021-02-07\n",
      "2021-02-08\n",
      "2021-02-09\n",
      "2021-02-10\n",
      "2021-02-11\n",
      "2021-02-12\n",
      "2021-02-13\n",
      "2021-02-14\n",
      "2021-02-15\n",
      "2021-02-16\n",
      "2021-02-17\n",
      "2021-02-18\n",
      "2021-02-19\n",
      "2021-02-20\n",
      "2021-02-21\n",
      "2021-02-22\n",
      "2021-02-23\n",
      "2021-02-24\n",
      "2021-02-25\n",
      "2021-02-26\n",
      "2021-02-27\n",
      "2021-02-28\n",
      "2021-03-01\n",
      "2021-03-02\n",
      "2021-03-03\n",
      "2021-03-04\n",
      "2021-03-05\n",
      "2021-03-06\n",
      "2021-03-07\n",
      "2021-03-08\n",
      "2021-03-09\n",
      "2021-03-10\n",
      "2021-03-11\n",
      "2021-03-12\n",
      "2021-03-13\n",
      "2021-03-14\n",
      "2021-03-15\n",
      "2021-03-16\n",
      "2021-03-17\n",
      "2021-03-18\n",
      "2021-03-19\n",
      "2021-03-20\n",
      "2021-03-21\n",
      "2021-03-22\n",
      "2021-03-23\n",
      "2021-03-24\n",
      "2021-03-25\n",
      "2021-03-26\n",
      "2021-03-27\n",
      "2021-03-28\n",
      "2021-03-29\n",
      "2021-03-30\n",
      "2021-03-31\n",
      "2021-04-01\n",
      "2021-04-02\n",
      "2021-04-03\n",
      "2021-04-04\n",
      "2021-04-05\n",
      "2021-04-06\n",
      "2021-04-07\n",
      "2021-04-08\n",
      "2021-04-09\n",
      "2021-04-10\n",
      "2021-04-11\n",
      "2021-04-12\n",
      "2021-04-13\n",
      "2021-04-14\n",
      "2021-04-15\n",
      "2021-04-16\n",
      "2021-04-17\n",
      "2021-04-18\n",
      "2021-04-19\n",
      "2021-04-20\n",
      "2021-04-21\n",
      "2021-04-22\n",
      "2021-04-23\n",
      "2021-04-24\n",
      "2021-04-25\n",
      "2021-04-26\n",
      "2021-04-27\n",
      "2021-04-28\n",
      "2021-04-29\n",
      "2021-04-30\n",
      "2021-05-01\n",
      "2021-05-02\n",
      "2021-05-03\n",
      "2021-05-04\n",
      "2021-05-05\n",
      "2021-05-06\n",
      "2021-05-07\n",
      "2021-05-08\n",
      "2021-05-09\n",
      "2021-05-10\n",
      "2021-05-11\n",
      "2021-05-12\n",
      "2021-05-13\n",
      "2021-05-14\n",
      "2021-05-15\n",
      "2021-05-16\n",
      "2021-05-17\n",
      "2021-05-18\n",
      "2021-05-19\n",
      "2021-05-20\n",
      "2021-05-21\n",
      "2021-05-22\n",
      "2021-05-23\n",
      "2021-05-24\n",
      "2021-05-25\n",
      "2021-05-26\n",
      "2021-05-27\n",
      "2021-05-28\n",
      "2021-05-29\n",
      "2021-05-30\n",
      "2021-05-31\n",
      "2021-06-01\n",
      "2021-06-02\n",
      "2021-06-03\n",
      "2021-06-04\n",
      "2021-06-05\n",
      "2021-06-06\n",
      "2021-06-07\n",
      "2021-06-08\n",
      "2021-06-09\n",
      "2021-06-10\n",
      "2021-06-11\n",
      "2021-06-12\n",
      "2021-06-13\n",
      "2021-06-14\n",
      "2021-06-15\n",
      "2021-06-16\n",
      "2021-06-17\n",
      "2021-06-18\n",
      "2021-06-19\n",
      "2021-06-20\n",
      "2021-06-21\n",
      "2021-06-22\n",
      "2021-06-23\n",
      "2021-06-24\n",
      "2021-06-25\n",
      "2021-06-26\n",
      "2021-06-27\n",
      "2021-06-28\n",
      "2021-06-29\n",
      "2021-06-30\n",
      "2021-07-01\n",
      "2021-07-02\n",
      "2021-07-03\n",
      "2021-07-04\n",
      "2021-07-05\n",
      "2021-07-06\n",
      "2021-07-07\n",
      "2021-07-08\n",
      "2021-07-09\n",
      "2021-07-10\n",
      "2021-07-11\n",
      "2021-07-12\n",
      "2021-07-13\n",
      "2021-07-14\n",
      "2021-07-15\n",
      "2021-07-16\n",
      "2021-07-17\n",
      "2021-07-18\n",
      "2021-07-19\n",
      "2021-07-20\n",
      "2021-07-21\n",
      "2021-07-22\n",
      "2021-07-23\n",
      "2021-07-24\n",
      "2021-07-25\n",
      "2021-07-26\n",
      "2021-07-27\n",
      "2021-07-28\n",
      "2021-07-29\n",
      "2021-07-30\n",
      "2021-07-31\n",
      "2021-08-01\n",
      "2021-08-02\n",
      "2021-08-03\n",
      "2021-08-04\n",
      "2021-08-05\n",
      "2021-08-06\n",
      "2021-08-07\n",
      "2021-08-08\n",
      "2021-08-09\n",
      "2021-08-10\n",
      "2021-08-11\n",
      "2021-08-12\n",
      "2021-08-13\n",
      "2021-08-14\n",
      "2021-08-15\n",
      "2021-08-16\n",
      "2021-08-17\n",
      "2021-08-18\n",
      "2021-08-19\n",
      "2021-08-20\n",
      "2021-08-21\n",
      "2021-08-22\n",
      "2021-08-23\n",
      "2021-08-24\n",
      "2021-08-25\n",
      "2021-08-26\n",
      "2021-08-27\n",
      "2021-08-28\n",
      "2021-08-29\n",
      "2021-08-30\n",
      "2021-08-31\n",
      "2021-09-01\n",
      "2021-09-02\n",
      "2021-09-03\n",
      "2021-09-04\n",
      "2021-09-05\n",
      "2021-09-06\n",
      "2021-09-07\n",
      "2021-09-08\n",
      "2021-09-09\n",
      "2021-09-10\n",
      "2021-09-11\n",
      "2021-09-12\n",
      "2021-09-13\n",
      "2021-09-14\n",
      "2021-09-15\n",
      "2021-09-16\n",
      "2021-09-17\n",
      "2021-09-18\n",
      "2021-09-19\n",
      "2021-09-20\n",
      "2021-09-21\n",
      "2021-09-22\n",
      "2021-09-23\n",
      "2021-09-24\n",
      "2021-09-25\n",
      "2021-09-26\n",
      "2021-09-27\n",
      "2021-09-28\n",
      "2021-09-29\n",
      "2021-09-30\n",
      "2021-10-01\n",
      "2021-10-02\n",
      "2021-10-03\n",
      "2021-10-04\n",
      "2021-10-05\n",
      "2021-10-06\n",
      "2021-10-07\n",
      "2021-10-08\n",
      "2021-10-09\n",
      "2021-10-10\n",
      "2021-10-11\n",
      "2021-10-12\n",
      "2021-10-13\n",
      "2021-10-14\n",
      "2021-10-15\n",
      "2021-10-16\n",
      "2021-10-17\n",
      "2021-10-18\n",
      "2021-10-19\n",
      "2021-10-20\n",
      "2021-10-21\n",
      "2021-10-22\n",
      "2021-10-23\n",
      "2021-10-24\n",
      "2021-10-25\n",
      "2021-10-26\n",
      "2021-10-27\n",
      "2021-10-28\n",
      "2021-10-29\n",
      "2021-10-30\n",
      "2021-10-31\n",
      "2021-11-01\n",
      "2021-11-02\n",
      "2021-11-03\n",
      "2021-11-04\n",
      "2021-11-05\n",
      "2021-11-06\n",
      "2021-11-07\n",
      "2021-11-08\n",
      "2021-11-09\n",
      "2021-11-10\n",
      "2021-11-11\n",
      "2021-11-12\n",
      "2021-11-13\n",
      "2021-11-14\n",
      "2021-11-15\n",
      "2021-11-16\n",
      "2021-11-17\n",
      "2021-11-18\n",
      "2021-11-19\n",
      "2021-11-20\n",
      "2021-11-21\n",
      "2021-11-22\n",
      "2021-11-23\n",
      "2021-11-24\n",
      "2021-11-25\n",
      "2021-11-26\n",
      "2021-11-27\n",
      "2021-11-28\n",
      "2021-11-29\n",
      "2021-11-30\n",
      "2021-12-01\n",
      "2021-12-02\n",
      "2021-12-03\n",
      "2021-12-04\n",
      "2021-12-05\n",
      "2021-12-06\n",
      "2021-12-07\n",
      "2021-12-08\n",
      "2021-12-09\n",
      "2021-12-10\n",
      "2021-12-11\n",
      "2021-12-12\n",
      "2021-12-13\n",
      "2021-12-14\n",
      "2021-12-15\n",
      "2021-12-16\n",
      "2021-12-17\n",
      "2021-12-18\n",
      "2021-12-19\n",
      "2021-12-20\n",
      "2021-12-21\n",
      "2021-12-22\n",
      "2021-12-23\n",
      "2021-12-24\n",
      "2021-12-25\n",
      "2021-12-26\n",
      "2021-12-27\n",
      "2021-12-28\n",
      "2021-12-29\n",
      "2021-12-30\n",
      "2021-12-31\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Guardian api\n",
    "# apply for your key here: https://open-platform.theguardian.com/access/\n",
    "MY_API_KEY = '1956fa1c-fa95-4b0b-a5a8-0436216ab868'\n",
    "\n",
    "# I followed this example:\n",
    "# code modified from https://gist.github.com/dannguyen/c9cb220093ee4c12b840\n",
    "\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "my_params = {\n",
    "    'from-date': \"\",\n",
    "    'to-date': \"\",\n",
    "    'order-by': \"newest\",\n",
    "    'show-fields': 'all',\n",
    "    'section':'film',\n",
    "    'page-size': 200,\n",
    "    'api-key': MY_API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "# check existance of tracker file, create if necessary\n",
    "if os.path.exists(\"progress_tracker.txt\") == False:\n",
    "    f = open(\"progress_tracker.txt\", \"w\")\n",
    "    f.close()\n",
    "    max_date = 0\n",
    "    start_date = date(2021,1,1)\n",
    "else:\n",
    "    f = open(\"progress_tracker.txt\", \"r\")\n",
    "    read_line = f.readlines()\n",
    "    if len(read_line) > 1:\n",
    "        print('Continued from interrupt')\n",
    "        max_date = max([int(i.strip('/n')) for i in read_line])+1\n",
    "        start_date = date(2021,1,1) + timedelta(days=max_date)\n",
    "    else:\n",
    "        max_date = 0\n",
    "        start_date = date(2021,1,1)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "end_date = date(2021,12,31)\n",
    "all_days = range((end_date - start_date).days + 1)\n",
    "all_results = []\n",
    "f = open(\"progress_tracker.txt\", \"a\")\n",
    "for day_count in all_days:\n",
    "    dt = start_date + timedelta(days=day_count)\n",
    "    if dt <= end_date:\n",
    "        print(dt)\n",
    "        f.write(str(day_count+max_date)+'\\n')\n",
    "        datestr = dt.strftime('%Y-%m-%d')\n",
    "        if dt == end_date:\n",
    "            f.truncate(0)\n",
    "        my_params['from-date'] = datestr\n",
    "        my_params['to-date'] = datestr\n",
    "        page_count = 1\n",
    "        num_pages = 1\n",
    "        while page_count <= num_pages:\n",
    "            my_params['page'] = page_count\n",
    "            resp = requests.get(API_ENDPOINT, my_params)\n",
    "            data = resp.json()\n",
    "            # [topic, text] list\n",
    "            all_results.extend([[i[\"sectionName\"], i['fields']['bodyText']] for i in data['response']['results']])\n",
    "            # save progress\n",
    "            pd.DataFrame([[i[\"sectionName\"], i['fields']['bodyText']] for i in data['response']['results']], \n",
    "                         columns = None, index = None).to_csv('progress_file.tsv', sep=\"\\t\", \n",
    "                                                              mode='a', index = False, header = 0)\n",
    "            # if there is more than one page, also look at other pages too\n",
    "            page_count += 1\n",
    "            num_pages = data['response']['pages']\n",
    "        time.sleep(1)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a63062-762f-4d4d-9e2c-074dd5916deb",
   "metadata": {},
   "source": [
    "In the code chunk below, there is some data pre-processing and statistics, we tokenize the texts using NLTK.word_tokenize, and only keep articles with less than 1000 words.\n",
    "Additionally, we store the articles and the token count(Length) in a pd dataframe, and remove duplicate rows if necessary, and only keep the first 500 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68af44f9-dede-4dd4-ab16-c45fde687829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In print, DC Comics was the trailblazer for su...</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desperately Seeking Susan Out with the old, in...</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s the “last day in paradise” for Las Vegas ...</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joan Micklin Silver, the American film-maker b...</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sylvie (Tessa Thompson) has been taught by her...</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>A sequel to the successful spin-off film from ...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The rescheduled 2021 edition of the Cannes fil...</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>In Morocco, sex outside marriage is punishable...</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Were there an Oscar for best on-screen drunk (...</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>There is nothing pure about Monster Trucks. It...</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Length\n",
       "0    In print, DC Comics was the trailblazer for su...     774\n",
       "1    Desperately Seeking Susan Out with the old, in...     701\n",
       "2    It’s the “last day in paradise” for Las Vegas ...     286\n",
       "3    Joan Micklin Silver, the American film-maker b...     585\n",
       "4    Sylvie (Tessa Thompson) has been taught by her...     302\n",
       "..                                                 ...     ...\n",
       "495  A sequel to the successful spin-off film from ...     186\n",
       "496  The rescheduled 2021 edition of the Cannes fil...     403\n",
       "497  In Morocco, sex outside marriage is punishable...     395\n",
       "498  Were there an Oscar for best on-screen drunk (...     555\n",
       "499  There is nothing pure about Monster Trucks. It...     929\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the results, and only keep the articles with less than 1000 words\n",
    "# convert pd df to list, for processing(tokenize the text)\n",
    "all_results = pd.read_csv('progress_file.tsv', sep = '\\t', header=None).values.tolist()\n",
    "final_results = []\n",
    "for topic, text in all_results:\n",
    "    tokenized = word_tokenize(text)\n",
    "    if len(tokenized) < 1000: #  including articles with less than 1000 words only\n",
    "        final_results.append([text, len(tokenized)])\n",
    "\n",
    "# conver to pd dataframe, to sort them by topics (optional)\n",
    "final_df = pd.DataFrame(final_results, columns = ['Text', 'Length'])\n",
    "# drop duplicates, if necessary\n",
    "final_df = final_df.drop_duplicates()\n",
    "# only keep the first 500 files \n",
    "final_df = final_df.head(500)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4339918-b14c-44e6-a954-cf3e7895401e",
   "metadata": {},
   "source": [
    "Assertions to check for duplicates and document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2be6bc-1774-43d6-bda7-b16bf37d335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicates!\n",
      "We have 500 articles!\n"
     ]
    }
   ],
   "source": [
    "# assertion to check there is no duplicates in text\n",
    "assert final_df['Text'].nunique() == len(final_df)\n",
    "print('There are no duplicates!') \n",
    "# assertion to check there is 500 articles\n",
    "assert len(final_df) == 500\n",
    "print('We have 500 articles!') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602f7b5-b2d4-4b6f-8ba7-a1bd4ee774d0",
   "metadata": {},
   "source": [
    "Finally, we save our data to *corpus.tsv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a07b452-c36c-4e15-9bcb-66b7cec90883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our final data in tsv file\n",
    "final_df.to_csv('corpus.tsv', sep=\"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963fecd-6498-4ebc-9345-8197c6917584",
   "metadata": {},
   "source": [
    "## 2. Corpus + explanation code\n",
    "Please see corpus_readme.md for detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e845c-b5b2-41e8-a267-1cecfa5db9f5",
   "metadata": {},
   "source": [
    "We calculate text statistics, including the total/average file length and the total/average number of distinct tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628f448e-a66c-4c66-8943-3ad6b4827b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = final_df.values.tolist()\n",
    "total_text = set()\n",
    "total_length = 0\n",
    "for text, word_count in final_result:\n",
    "    tokenized = word_tokenize(text)\n",
    "    total_text.update(set(tokenized))\n",
    "    total_length += len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df437a02-70f3-4afb-81fd-dd178c1ba883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total text length: 269519\n",
      "The total distinct number of tokens: 28051\n",
      "Average article length in words: 539.04\n",
      "Average article length in distinct tokens: 56.1\n"
     ]
    }
   ],
   "source": [
    "print('The total text length:', total_length)\n",
    "print('The total distinct number of tokens:', len(total_text))\n",
    "print('Average article length in words:', round(total_length/len(final_df), 2))\n",
    "print('Average article length in distinct tokens:', round(len(total_text)/len(final_df), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247ea5d-9246-4b5d-9137-644be6adc0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
